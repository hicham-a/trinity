#!/usr/bin/env bash
##title          : cv_install_openstack_on_controller
##description    : Setup the openstack dashboard + trinity_api services baremetal on controller
##                   Dashboard (Openstack Horizon)
##                   Trinity API
##                   Trinity client
##                   Trinity dashboard additions
##                 Also the first tenant is created and the first virtual cluster is setup 
##author         : Abhishek Mukherjee, Hans Then
##email          : abhishek.mukherjee@clustervision.com, hans.then@clustervision.com

#------------------------------------------------------------------------------
# Install and configure Horizon 
#------------------------------------------------------------------------------
yum -y -q install --setopt=tsflags=nodocs python-keystoneclient python-glanceclient python-novaclient python-cinderclient
yum -y -q install --setopt=tsflags=nodocs openstack-dashboard httpd mod_wsgi memcached python-memcached

cp -LrT /trinity/horizon/rootimg/ /
chown -R apache:apache /usr/share/openstack-dashboard/static
echo 'Include "/etc/httpd/conf.modules.d/*.conf"' >> /etc/httpd/conf/httpd.conf
systemctl enable httpd.service memcached.service
systemctl start memcached.service
systemctl restart httpd.service

#-----------------------------------------------------------------
# Install the API server, client and dashboard additions
#-----------------------------------------------------------------

cp -LrT /trinity/openstack/rootimg/ /
# Assumes that /cluster and  /home are already mounted

# The Trinity API server init script requires the Linux Standard Base
yum -y -q install redhat-lsb

#-- The Trinity API server needs python Bottle
yum -y -q install epel-release
yum -y -q install python-pip
pip install bottle
pip install cherrypy
pip install tzlocal

#-- Trinity API setup
cd /opt/trinity/trinity_api
python setup.py install
cd -
if [[ ! -z "${SITEMASTER}" ]]; then
    sed 's#/trinity/login/setup.sh#/trinity/login/mini-setup.sh#' -i /etc/trinity/trinity_api.conf
fi 

BRIDGE="br100"
source /etc/profile.d/xcat.sh

sed -e "s/127.0.0.1/10.141.255.254/g" \
    -e "s/eno2/${BRIDGE}/g" \
    -i /etc/trinity/trinity_api.conf


mkdir -p /var/log/trinity

# Start the API
systemctl start trinity-api
systemctl enable trinity-api

#-- Install the python client
cd /opt/trinity/trinity_client
python setup.py install
cd -
sed -e "s/127.0.0.1/10.141.255.254/g" \
    -i /etc/trinity/trinity_client.conf

#-- Install the dashboard additions
cd /opt/trinity/trinity_dashboard
sh setup.sh
cd -

service httpd restart


#-----------------------------------------------------------------
# Install the first cluster
#-----------------------------------------------------------------

sed -e "s/<TENANT>/admin/g" \
    -e "s/<USER>/admin/g" \
    -e "s/<PASSWORD>/system/g" \
    -e "s/<OPENSTACK>/10.141.255.254/g" \
    /trinity/openstack/keystonerc > /root/keystonerc_admin
source /root/keystonerc_admin


#-- Download the base login node image (CentOS cloud image) and upload it to glance
if [[ -z "$SITEMASTER" ]]; then
    CLOUD_IMAGE_URL="http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2"
    CLOUD_IMAGE="/tmp/CentOS-7-x86_64-GenericCloud.qcow2"
    CLOUD_IMAGE_FORMAT="qcow2"
    CLOUD_CONTAINER_FORMAT="bare"
    LOGIN_IMAGE="login"
    LOGIN_DATA="/root/.login_data"
    LOGIN_SETUP="/trinity/login/setup.sh"
    curl -s ${CLOUD_IMAGE_URL} -o ${CLOUD_IMAGE}
else
    # We have a sitemaster, copy from master node.
    mkdir -p /trinity/qcows/
    CLOUD_IMAGE="/trinity/qcows/login.qcow2"
    # FIXME: Find a better location. We cannot assume /tmp/trinity is still mounted here.
    cp /tmp/trinity/qcows/login.qcow2 ${CLOUD_IMAGE}
    CLOUD_IMAGE_FORMAT="qcow2"
    CLOUD_CONTAINER_FORMAT="bare"
    LOGIN_IMAGE="login"
    LOGIN_DATA="/root/.login_data"
    LOGIN_SETUP="/trinity/login/mini-setup.sh"
fi

glance image-create \
    --name "${LOGIN_IMAGE}" \
    --disk-format "${CLOUD_IMAGE_FORMAT}" \
    --container-format "${CLOUD_CONTAINER_FORMAT}" \
    --is-public True \
    --file "${CLOUD_IMAGE}"

#-- Create the Flat Network
read ETH0 ETH1 ETH2 <<<$(ls /sys/class/net/ | grep "^e" | sort | head -3)
PRI_NIC=${ETH1} 

NETWORK_LABEL="novanetwork"
nova network-create \
       --fixed-range-v4 192.168.32.0/24 \
       --fixed-cidr 192.168.32.0/24 \
       --bridge-interface ${PRI_NIC} \
       --bridge ${BRIDGE} \
       ${NETWORK_LABEL}
       

#-- Create the default cluster
FIRST_TENANT="a"
FIRST_USER="a"
FIRST_PASS="system"
FIRST_CLUSTER="vc-a"
FIRST_POOL="login-a"
FIRST_FLOATING="172.16.255.254"
LOGIN_NODE="login-a"
SKEL=".skel"

keystone tenant-create --name ${FIRST_TENANT}
keystone user-create --name ${FIRST_USER} --tenant ${FIRST_TENANT} --pass ${FIRST_PASS}
nova floating-ip-bulk-create --pool ${FIRST_POOL} --interface ${BRIDGE} ${FIRST_FLOATING}
mkdir -p /cluster/${FIRST_CLUSTER}
cp -Tr /cluster/${SKEL}/ /cluster/${FIRST_CLUSTER}/
mkdir -p /home/${FIRST_CLUSTER}

useradd -u 1002 -U munge --no-create-home
mkdir -p /cluster/${FIRST_CLUSTER}/etc/munge
[ -e  /cluster/${FIRST_CLUSTER}/etc/munge/munge.key ] && rm /cluster/${FIRST_CLUSTER}/etc/munge/munge.key
dd if=/dev/urandom bs=1 count=1024 > /cluster/${FIRST_CLUSTER}/etc/munge/munge.key
chown -R munge:munge /cluster/${FIRST_CLUSTER}/etc/munge
chmod u=rwx,go= /cluster/${FIRST_CLUSTER}/etc/munge
chmod u=r,go=   /cluster/${FIRST_CLUSTER}/etc/munge/munge.key

# nodes=$(curl -k 'https://controller/xcatws/groups/vc-a/attrs/members?userName=trinity&password=trinity&pretty=1' | awk -F: '{if (NR==3) {print $2}}' | sed 's/"//g')
# sed -e "s/<NODES>/${nodes}/g" -i /cluster/${FIRST_CLUSTER}/etc/slurm/slurm-nodes.conf
#-- Empty out the slurm-nodes file
> /cluster/${FIRST_CLUSTER}/etc/slurm/slurm-nodes.conf
chmod u=rwx,go=rx /cluster/${FIRST_CLUSTER}/etc/slurm
chmod u=rw,go=rx  /cluster/${FIRST_CLUSTER}/etc/slurm/slurm.conf
chmod u=rw,go=r   /cluster/${FIRST_CLUSTER}/etc/slurm/slurm-nodes.conf
chmod ug=rw,o=r   /cluster/${FIRST_CLUSTER}/etc/slurm/slurm-user.conf

cp ${LOGIN_SETUP} ${LOGIN_DATA}

sed -e "s/FLOATING_IP=127.0.0.1/FLOATING_IP=${FIRST_FLOATING}/g" \
    -e "s/vc-a/${FIRST_CLUSTER}/g" \
    -i "${LOGIN_DATA}"

sed -e "s/<TENANT>/${FIRST_TENANT}/g" \
    -e "s/<USER>/${FIRST_USER}/g" \
    -e "s/<PASSWORD>/${FIRST_PASS}/g" \
    -e "s/<OPENSTACK>/$(hostname -i)/g" \
    /trinity/openstack/keystonerc > /root/keystonerc_${FIRST_TENANT}
source /root/keystonerc_${FIRST_TENANT}

nova floating-ip-create ${FIRST_POOL}
nova secgroup-add-rule default tcp 1 65535 0.0.0.0/0
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0
nova boot --flavor 2 --image ${LOGIN_IMAGE} --security-groups default --user-data ${LOGIN_DATA} ${LOGIN_NODE}

sleep 5
nova floating-ip-associate ${LOGIN_NODE} ${FIRST_FLOATING}

#--------------------------------------------------------------------------------------
# setup routing for the virtual clusters
#--------------------------------------------------------------------------------------
ip route add 172.16.0.0/12 dev ${BRIDGE}
systemctl enable trinity-route
systemctl start trinity-route
#if [[ -f /etc/sysconfig/network-scripts/route-${ETH1} ]]; then
#   mv /etc/sysconfig/network-scripts/route-${ETH1} /etc/sysconfig/network-scripts/route-${BRIDGE}
#fi

# Provide DNS on the bridge interface not the physical interface.
while ! (ip a show dev ${BRIDGE} | grep 10.141.255.254); do sleep 10; done
chtab key=dnsinterfaces site.value=${BRIDGE}
chtab key=dhcpinterfaces site.value=${BRIDGE}
chtab netname=internal_net networks.mgtifname=${BRIDGE}
chtab netname=bmc_net networks.mgtifname=${BRIDGE}
chtab netname=vc_a_net networks.mgtifname=${BRIDGE}
makedhcp -n
makedns -n

echo "$0 finished @ $(date)" >> /var/log/postinstall.log


