#! /usr/bin/bash
##description    : Setup the first round of High Availability.
##	           Basic pacemaker and xCAT configuration required on both nodes.
##author         : Hans Then 
##email          : hans.then@clustervision

#-----------------------------------------------------------------------------------------------
# Determine if this will be the active or passive node. An odd node will become 
# the active node. Any even node will become the passive node. 
#
# Adapt the network configuration accordingly.
#-----------------------------------------------------------------------------------------------
n=$(echo $NODE | egrep -o '[[:digit:]]*' | head -n1)
# normalize to 1 or 2
N=$((((n-1)%2)+1))
# set the new ip addresses
octet=$((254-$N))

set -x
echo $PACEMAKER_PW | passwd hacluster --stdin
echo drbd > /etc/modules-load.d/drbd.conf

#------------------------------------------------------------
# configure drbd
#------------------------------------------------------------
# remove the drbd volume from mounting
sed -i -e "/vg_root-lv_drbd/d" /etc/fstab
umount /drbd

# Clear the disk
echo Clearing drbd logical volume, this may take some time.
dd if=/dev/zero of=/dev/mapper/vg_root-lv_drbd bs=1M count=1024

# make sure the host names can be resolved
if ! grep $CONTROLLER_ACTIVE_IP /etc/hosts; then echo "$CONTROLLER_ACTIVE_IP controller-1 controller-1.$CONTROLLER_DOMAIN" >> /etc/hosts; fi
if ! grep $CONTROLLER_PASSIVE_IP /etc/hosts; then echo "$CONTROLLER_PASSIVE_IP controller-2 controller-2.$CONTROLLER_DOMAIN" >> /etc/hosts; fi
if ! grep $CONTROLLER_FIP /etc/hosts; then echo "$CONTROLLER_FIP controller controller.$CONTROLLER_DOMAIN" >> /etc/hosts; fi
hostnamectl set-hostname controller-${N}.$CONTROLLER_DOMAIN

# If we can use the IB interface for DRBD we will do so.
if ping -c1 $CONTROLLER_ACTIVE_IB_IP > /dev/null 2>&1; then
    ip1=$CONTROLLER_ACTIVE_IB_IP
    ip2=$CONTROLLER_PASSIVE_IB_IP
else
    ip1=$CONTROLLER_ACTIVE_IP
    ip2=$CONTROLLER_PASSIVE_IP
fi

cat <<EOF >/etc/drbd.d/ha_disk.res
resource ha_disk {
       net {
         after-sb-0pri discard-younger-primary;
         after-sb-1pri discard-secondary;
         after-sb-2pri disconnect;
       }
       on controller-1.$CONTROLLER_DOMAIN {
         device    /dev/drbd1;
         disk      /dev/mapper/vg_root-lv_drbd;
         address   ${ip1}:7789;
         meta-disk internal;
       }
       on controller-2.$CONTROLLER_DOMAIN {
         device    /dev/drbd1;
         disk      /dev/mapper/vg_root-lv_drbd;
         address   ${ip2}:7789;
         meta-disk internal;
       }
}
EOF

drbdadm create-md ha_disk
modprobe drbd
drbdadm up ha_disk

#-----------------------------------------------------------------------------------------------
# Install pacemaker
#-----------------------------------------------------------------------------------------------
systemctl start pcsd
systemctl enable pcsd

pcs cluster auth $(hostname) -u hacluster -p $PACEMAKER_PW
if [[ $N = 1 ]]; then
    pcs cluster setup --name ha_cluster $(hostname)
else
    for i in {1..100}; do
        ssh controller-1 pcs status && break
        echo "waiting for master to come up"
        sleep 1m
    done
    [[ $i = 100 ]] && echo "TIMEOUT: waiting for master node to come up"
    ssh controller-1.$CONTROLLER_DOMAIN pcs cluster auth $(hostname) -u hacluster -p $PACEMAKER_PW
    ssh controller-1.$CONTROLLER_DOMAIN pcs cluster node add $(hostname)
    pcs cluster auth
    scp -r controller-1.$CONTROLLER_DOMAIN/root/.xcat /root
fi
pcs cluster start

systemctl enable pacemaker
systemctl enable corosync
systemctl enable opensm
systemctl start opensm
systemctl enable ntpd
systemctl start ntpd

#----------------------------------------------------------------------------------
# xCAT setup required on both nodes
#----------------------------------------------------------------------------------
systemctl stop NetworkManager
systemctl disable NetworkManager
killall dhclient

# Fix for #356
systemctl disable firewalld
systemctl stop firewalld

# setup NAT, so nodes can access the internet (see manual step 1.f)
systemctl enable iptables 
systemctl start iptables
modprobe iptable_nat

iptables -A FORWARD -i $CONTROLLER_CLUSTER_NIC -j ACCEPT
iptables -t nat -A POSTROUTING -o $CONTROLLER_PUBLIC_NIC -j MASQUERADE
iptables -D FORWARD -j REJECT --reject-with icmp-host-prohibited
iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited
service iptables save

#--------------------------------------------------------------------------------------
# setup routing for the virtual clusters
#--------------------------------------------------------------------------------------
ip route add 172.16.0.0/12 dev $CONTROLLER_CLUSTER_NIC
echo "172.16.0.0/12 dev $CONTROLLER_CLUSTER_NIC" > /etc/sysconfig/network-scripts/route-$CONTROLLER_CLUSTER_NIC

if ! systemctl restart network; then
    sleep 3
    systemctl start network
fi

setenforce 0
sestatus

#-----------------------------------------------------------------------------------------
# Patch xCAT. Needs to be done on both nodes.
#-----------------------------------------------------------------------------------------
if [ ! -z $SITEMASTER ]; then
    mkdir $CONTROLLER_MASTER_FS
    mount ${SITEMASTER}:/trinity $CONTROLLER_MASTER_FS
fi

cd /opt/xcat/share/xcat/netboot/centos
ln -s ../rh/dracut_033 .
ln -s ../rh/dracut .
cd /
cat $CONTROLLER_MASTER_FS/controller/xcat/patches/*.patch | patch -p0
systemctl restart xcatd

#--------------------------------------------------------------------------------------
# copy required files from the master node to the controller node
#--------------------------------------------------------------------------------------
cp -LrT $CONTROLLER_MASTER_FS/controller/rootimg /
cp $CONTROLLER_MASTER_FS/version /trinity/version
if [[ -f $CONTROLLER_MASTER_FS/site ]]; then
    cp $CONTROLLER_MASTER_FS/site /trinity/site
fi
mkdir -p /var/lib/docker-registry

#------------------------------------------------------------------------------
# Install and configure Horizon
#------------------------------------------------------------------------------
cp -LrT /trinity/horizon/rootimg/ /
chown -R apache:apache /usr/share/openstack-dashboard/static
if ! grep conf.modules.d/ /etc/httpd/conf/httpd.conf; then
    echo 'Include "/etc/httpd/conf.modules.d/*.conf"' >> /etc/httpd/conf/httpd.conf
fi

#-----------------------------------------------------------------
# Install the API server, client and dashboard additions
#-----------------------------------------------------------------
cp -LrT /trinity/openstack/rootimg/ /

#-- Trinity API setup
cd /opt/trinity/trinity_api
python setup.py install
cd -

#Install the python client
cd /opt/trinity/trinity_client
python setup.py install
cd -

#-- Install the dashboard additions
cd /opt/trinity/trinity_dashboard
sh setup.sh
cd -

set -x
# FIXME: use obol for this.
useradd -u 1002 -U munge --no-create-home

mkdir -p /var/log/trinity
# FIXME: rather hackish, ensure this directory exists on both nodes.
mkdir -p /nfshome
systemctl daemon-reload

#---------------------------------------------------------------------------------------
# Set up the xCAT web service
#---------------------------------------------------------------------------------------
source /etc/profile.d/xcat.sh
export HOME=/root
# FIXME: use obol for this.
useradd trinity
echo $TRINITY_USER_PW | passwd trinity --stdin
mkdef -t policy 6 name=trinity rule=allow
sed  -e "/<Directory \/>/,/<\/Directory>/ s/Require/#Require/" -i /etc/httpd/conf/httpd.conf
systemctl restart httpd 

#Make xCAT available via the floating IP.
if grep -v XCATHOST /etc/profile.d/xcat.sh; then
    echo "export XCATHOST=controller.$CONTROLLER_DOMAIN:3001" >> /etc/profile.d/xcat.sh
fi

systemctl enable replicator.timer
systemctl start replicator.timer
systemctl enable memcached
systemctl start memcached

chmod -x /usr/lib/systemd/system/*

echo "$0 finished @ $(date)" >> /var/log/postinstall.log
